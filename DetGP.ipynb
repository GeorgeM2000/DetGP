{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/DetGP/blob/master/DetGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa86248",
      "metadata": {
        "id": "dfa86248"
      },
      "source": [
        "# ***Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "89ff0f1c",
      "metadata": {
        "id": "89ff0f1c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import networkx as nx\n",
        "import tensorflow as tf\n",
        "import scipy.cluster\n",
        "import gc\n",
        "import math\n",
        "import shutil\n",
        "import re\n",
        "import os\n",
        "\n",
        "from tensorflow.sparse import sparse_dense_matmul as sd_matmul\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9757d0b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9757d0b2",
        "outputId": "35d994d6-67a8-4096-dfc2-d7dae439da24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs Available:  1\n",
            "GPU is ready for use!\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Optional: Set GPU memory growth to avoid over-allocation\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU is ready for use!\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3536e3d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3536e3d1",
        "outputId": "be0f79be-7533-42fb-e896-8398d42e3e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is TensorFlow using GPU? True\n",
            "GPU device: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "print(\"Is TensorFlow using GPU?\", tf.test.is_built_with_cuda())  # Should return True\n",
        "print(\"GPU device:\", tf.test.gpu_device_name())  # Should return something like /device:GPU:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "40f05ae0",
      "metadata": {
        "id": "40f05ae0"
      },
      "outputs": [],
      "source": [
        "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "nuTWRkRZI-T-"
      },
      "id": "nuTWRkRZI-T-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5149d36a",
      "metadata": {
        "id": "5149d36a"
      },
      "source": [
        "# ***Variables & General Functionality***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "35ed8cbb",
      "metadata": {
        "id": "35ed8cbb"
      },
      "outputs": [],
      "source": [
        "dataset_name    = \"arxiv\"\n",
        "\n",
        "parent_path     = f'Datasets/{dataset_name}/graph-v2'\n",
        "parent_path     = '/content'\n",
        "\n",
        "\n",
        "data_text_file  = \"data.txt\"\n",
        "data_text_files = [\"data-v3-500.txt\",\n",
        "                   \"data-v3-500C.txt\",\n",
        "                   \"YAKE10.txt\",\n",
        "                   \"YAKE5.txt\",\n",
        "                   \"RAKE10.txt\",\n",
        "                   \"RAKE5.txt\",\n",
        "                   \"RAKE10C.txt\",\n",
        "                   \"RAKE5C.txt\",\n",
        "                   \"TFIDF10.txt\",\n",
        "                   \"TFIDF5.txt\",\n",
        "                   \"PosR5.txt\",\n",
        "                   \"PosR10.txt\",\n",
        "                   \"TextR5.txt\",\n",
        "                   \"TextR10.txt\",\n",
        "                   \"TopicR5.txt\",\n",
        "                   \"TopicR10.txt\"]\n",
        "\n",
        "graph_file      = 'graph.txt'\n",
        "categories_file = 'group-v3.txt'\n",
        "\n",
        "log_file               = 'DetGP_Execution_Logs.txt'\n",
        "link_pred_results_file = 'DetGP_Link_Pred_Res.txt'\n",
        "node_clf_results_file  = 'DetGP_Node_Clf_Res.txt'\n",
        "\n",
        "\n",
        "split_graph_file  = 'sgraph75.txt'\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "\n",
        "test_graph_file   = 'tgraph25.txt'\n",
        "test_graph_files  = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "75273cbc",
      "metadata": {
        "id": "75273cbc"
      },
      "outputs": [],
      "source": [
        "# Parameters for node classification\n",
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num   = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "94b6de36",
      "metadata": {
        "id": "94b6de36"
      },
      "outputs": [],
      "source": [
        "MAX_LEN          = 300\n",
        "MAX_LENS         = []\n",
        "neg_table_size   = 1000000\n",
        "NEG_SAMPLE_POWER = 0.75\n",
        "batch_size       = 64 # Orig: 128\n",
        "num_epoch        = 50\n",
        "embed_size       = 200\n",
        "lr               = 1e-3\n",
        "inducing_num     = 20\n",
        "random_seed      = 42\n",
        "encoder_type     = 'wavg'  # 'dmte' 'wavg'\n",
        "kernel_type      = 'linear'\n",
        "model_name       = 'DetGP'\n",
        "trans_order      = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "40ceccff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ceccff",
        "outputId": "1918c4e2-f024-4ab0-c9e4-09320b20adc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== data.txt ===\n",
            "Mean word count: 91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find the average number of words from each data text file\n",
        "for txtf in ['data.txt']: # Options: 1) ['data.txt'] for one data text file 2) data_text_files for multiple files\n",
        "    total_word_count = 0\n",
        "    total_lines = 0\n",
        "    with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as f:\n",
        "        for line in f: # A line represents a node's content\n",
        "            total_word_count += len(re.findall(r\"\\b\\w+\\b\", line))\n",
        "            total_lines += 1\n",
        "\n",
        "    mean_word_count = total_word_count / total_lines if total_lines > 0 else 0\n",
        "    MAX_LENS.append(int(math.ceil(mean_word_count)))\n",
        "    print(f'=== {txtf} ===')\n",
        "    print(f'Mean word count: {math.ceil(mean_word_count)}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b1dac2b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1dac2b8",
        "outputId": "131f352b-3afc-44d5-a7cd-70d4cc1a6f36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "MAX_LENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4b68be1f",
      "metadata": {
        "id": "4b68be1f"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For one execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8d44f8b8",
      "metadata": {
        "id": "8d44f8b8"
      },
      "outputs": [],
      "source": [
        "zero_list = np.array([0 for _ in range(embed_size)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "80f8ed04",
      "metadata": {
        "id": "80f8ed04"
      },
      "outputs": [],
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "  with open(f'{file_path}', \"r\") as f:\n",
        "      for idx, line in enumerate(f): # Each line has the embedding of a node\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7775a009",
      "metadata": {
        "id": "7775a009"
      },
      "source": [
        "# ***TOOLS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "48de3cf2",
      "metadata": {
        "id": "48de3cf2"
      },
      "outputs": [],
      "source": [
        "def edges_to_undirect(node_num, edges):\n",
        "    # When given edge [i,j] build undirect graph with edge [i,j] and [j,i], each edge only appears once\n",
        "    # Create unique hash codes for the nodes in the edges\n",
        "    edge_hash = []\n",
        "    for edge in edges:\n",
        "        edge_hash.append(edge[0] * node_num + edge[1])\n",
        "        edge_hash.append(edge[1] * node_num + edge[0])\n",
        "\n",
        "    edge_hash = list(set(edge_hash)) # edge_hash will have unique hash codes for the nodes\n",
        "    ud_edges = [[v // node_num, v % node_num] for v in edge_hash]\n",
        "    return ud_edges\n",
        "\n",
        "def get_initial_inducing(sess, model, inducing_num):\n",
        "    text_feature = sess.run(model.text_feature)\n",
        "    inducing_points = scipy.cluster.vq.kmeans2(text_feature, inducing_num, minit='points')[0]\n",
        "    return inducing_points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b2c562",
      "metadata": {
        "id": "42b2c562"
      },
      "source": [
        "# ***NEGATIVE TABLE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b32b8757",
      "metadata": {
        "id": "b32b8757"
      },
      "outputs": [],
      "source": [
        "def InitNegTable(edges):\n",
        "    a_list, b_list = zip(*edges)\n",
        "    a_list = list(a_list)\n",
        "    b_list = list(b_list)\n",
        "    node = a_list\n",
        "    node.extend(b_list)\n",
        "\n",
        "    node_degree = {}\n",
        "    for i in node:\n",
        "        if i in node_degree:\n",
        "            node_degree[i] += 1\n",
        "        else:\n",
        "            node_degree[i] = 1\n",
        "    sum_degree = 0\n",
        "    for i in node_degree.values():\n",
        "        sum_degree += pow(i, 0.75)\n",
        "\n",
        "    por = 0\n",
        "    cur_sum = 0\n",
        "    vid = -1\n",
        "    neg_table = []\n",
        "    degree_list = list(node_degree.values())\n",
        "    node_id = list(node_degree.keys())\n",
        "    for i in range(neg_table_size):\n",
        "        if ((i + 1) / float(neg_table_size)) > por:\n",
        "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
        "            por = cur_sum / sum_degree\n",
        "            vid += 1\n",
        "        neg_table.append(node_id[vid])\n",
        "\n",
        "    return neg_table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1daf22ca",
      "metadata": {
        "id": "1daf22ca"
      },
      "source": [
        "# ***DATALOADER***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5e1b26dc",
      "metadata": {
        "id": "5e1b26dc"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, text_path, graph_path, edge_split_ratio=None):\n",
        "        #self.graph, self.train_graph, self.test_graph = self.load_graph(graph_path, edge_split_ratio)\n",
        "        text_file, graph_file = self.load(text_path, graph_path)\n",
        "        self.nodes = []\n",
        "        self.edges = self.load_edges(graph_file)\n",
        "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "        #self.train_edges = edges_to_undirect(self.num_nodes, self.train_graph.edges())\n",
        "        #self.test_edges = edges_to_undirect(self.num_nodes, self.test_graph.edges())\n",
        "        self.negative_table = InitNegTable(self.edges)\n",
        "\n",
        "    def load(self, text_path, graph_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        for a in range(len(text_file)):\n",
        "            text_file[a] = str(text_file[a])\n",
        "\n",
        "        return text_file, open(graph_path, 'rb').readlines()\n",
        "\n",
        "    def load_edges(self, graph_file):\n",
        "        edges = []\n",
        "        for edge in graph_file:\n",
        "            edges.append(list(map(int, edge.strip().decode().split())))\n",
        "\n",
        "        for edge in edges:\n",
        "            for node in edge:\n",
        "                if node not in self.nodes:\n",
        "                    self.nodes.append(node)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "        print(\"Total edges: %d\" % len(edges))\n",
        "        return edges\n",
        "\n",
        "    def get_adj_list(self, edges, node_num):\n",
        "        adj_list = []\n",
        "        for node in range(node_num):\n",
        "                nbr = [node]\n",
        "                for edge in edges:\n",
        "                        if node in edge:\n",
        "                                nbr += edge\n",
        "                adj_list.append(list(set(nbr)))\n",
        "        return adj_list\n",
        "\n",
        "\n",
        "    def load_graph(self, graph_path, edge_split_ratio):\n",
        "        total_graph = nx.Graph()\n",
        "        train_graph = nx.Graph()\n",
        "        test_graph = nx.Graph()\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "        for edge in graph_file:\n",
        "            edge = list(map(int, edge.strip().decode().split()))\n",
        "            total_graph.add_edge(edge[0], edge[1])\n",
        "\n",
        "            if np.random.uniform(0.0, 1.0) <= edge_split_ratio:\n",
        "                train_graph.add_edge(edge[0], edge[1])\n",
        "            else:\n",
        "                test_graph.add_edge(edge[0], edge[1])\n",
        "\n",
        "        return total_graph, train_graph, test_graph\n",
        "\n",
        "    # Original load_text() method\n",
        "    \"\"\" def load_text(self, text_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        vocab = learn.preprocessing.VocabularyProcessor(MAX_LEN)\n",
        "        return np.array(list(vocab.fit_transform(text_file))), len(vocab.vocabulary_), len(text_file) \"\"\"\n",
        "\n",
        "    def load_text(self, text_file):\n",
        "        #text_file = open(text_path, 'rb').readlines()\n",
        "        #for a in range(0, len(text_file)): text_file[a] = str(text_file[a])\n",
        "\n",
        "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=None,  # Set a limit if needed\n",
        "            output_mode='int',\n",
        "            output_sequence_length=MAX_LEN\n",
        "        )\n",
        "        text_data = [line.strip() for line in text_file]\n",
        "        vectorize_layer.adapt(text_data)\n",
        "        return vectorize_layer(text_data).numpy(), len(vectorize_layer.get_vocabulary()), len(text_data)\n",
        "\n",
        "\n",
        "    def subgraph_edges(self, node_list):\n",
        "        subg_edges = self.train_graph.subgraph(node_list).edges()\n",
        "        return edges_to_undirect(self.num_nodes, subg_edges)\n",
        "\n",
        "    # Original negative_sampling() method\n",
        "    \"\"\" def negative_sampling(self, graph, edges):\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "        #np.random.seed(config.random_seed)\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = random.choice(list(graph.nodes))\n",
        "            while neg_node in graph.neighbors(node1[i]):\n",
        "                neg_node = random.choice(list(graph.nodes))\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "        return sample_edges \"\"\"\n",
        "\n",
        "    def negative_sampling(self, edges):\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)]\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = func()\n",
        "            while node1[i] == neg_node or node2[i] == neg_node:\n",
        "                neg_node = func()\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "\n",
        "        return sample_edges\n",
        "\n",
        "\n",
        "    def generate_batches(self, mode=None):\n",
        "        num_batch = len(self.edges) // batch_size\n",
        "        edges = self.edges\n",
        "\n",
        "        if mode == 'add':\n",
        "          num_batch += 1\n",
        "          edges.extend(edges[:(batch_size - len(self.edges) // batch_size)])\n",
        "\n",
        "        if mode != 'add':\n",
        "          random.shuffle(edges)\n",
        "\n",
        "        sample_edges = edges[:num_batch*batch_size]\n",
        "        sample_edges = self.negative_sampling(sample_edges)\n",
        "\n",
        "        batches = [sample_edges[i * batch_size:(i + 1) * batch_size] for i in range(num_batch)]\n",
        "        return batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af19bc1c",
      "metadata": {
        "id": "af19bc1c"
      },
      "source": [
        "# ***MODEL***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2bd3e22d",
      "metadata": {
        "id": "2bd3e22d"
      },
      "outputs": [],
      "source": [
        "def RBF_Kernel(feature_a, feature_b, gamma = 1000.):\n",
        "    f_a = tf.expand_dims(feature_a, 1)  #[node_num_a, 1, f_dim]\n",
        "    f_b = tf.expand_dims(feature_b, 0)  #[1, node_num_b, f_dim]\n",
        "    k_ab = tf.reduce_sum(tf.square(f_a-f_b), axis = 2)\n",
        "    return tf.exp(-k_ab / gamma)\n",
        "\n",
        "def Linear_Kernel(feature_a, feature_b):\n",
        "    k_ab = tf.matmul(feature_a, feature_b, transpose_b = True)\n",
        "    return k_ab + 1.\n",
        "\n",
        "\n",
        "def Polynomial_Kernel(feature_a, feature_b):\n",
        "    return Linear_Kernel(feature_a, feature_b)**2.\n",
        "\n",
        "\n",
        "def regularized_adj(adj):\n",
        "    return adj/tf.sparse.reduce_sum(adj, axis = 1, keepdims = True)\n",
        "\n",
        "\n",
        "def get_transition_matrix(Adj): # a sparse adjacency matrix\n",
        "    reg_Adj = tf.sparse.add(Adj, tf.sparse.eye(tf.shape(Adj)[0]))\n",
        "    return reg_Adj/tf.sparse.reduce_sum(reg_Adj,axis = 1, keepdims = True)\n",
        "\n",
        "\n",
        "def norm_trans(adj_mat):\n",
        "    return adj_mat / tf.sparse.reduce_sum(adj_mat, axis = 1, keepdims = True)\n",
        "\n",
        "\n",
        "class DetGP():\n",
        "    def __init__(self, vocab_size, num_nodes, text_data, edges):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.kernel = Linear_Kernel\n",
        "        self.inducing_num = inducing_num\n",
        "        self.whiten = False\n",
        "        self.embedding_dim = embed_size // 2\n",
        "        self.trans_order = trans_order\n",
        "\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "            self.text_all = tf.constant(text_data, dtype = tf.int32)\n",
        "            self.node_a_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'a_ids')\n",
        "            self.node_b_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'b_ids')\n",
        "            self.node_n_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'n_ids')\n",
        "\n",
        "            self.edges = tf.constant(edges, dtype=tf.int64)\n",
        "            self.adj_mat = tf.sparse.SparseTensor(\n",
        "                            self.edges, tf.ones(tf.shape(self.edges)[0]),\n",
        "                            dense_shape=[self.num_nodes, self.num_nodes])\n",
        "            self.trans_mat = get_transition_matrix(self.adj_mat)\n",
        "            #self.initial_inducing_points = tf.compat.v1.placeholder(tf.float32, [None, embed_size // 2], name = 'inducing')\n",
        "\n",
        "        with tf.compat.v1.variable_scope('ggp') as scope:\n",
        "            self.inducing_points = tf.compat.v1.get_variable(name = 'inducing_points', shape = [self.inducing_num, self.embedding_dim])\n",
        "            self.q_mu            = tf.compat.v1.get_variable(name = 'embedding_mu', shape = [self.inducing_num, self.embedding_dim], initializer = tf.initializers.constant(0.1))\n",
        "            self.alpha           = tf.compat.v1.get_variable(name = 'alpha', shape = [trans_order + 1], initializer = tf.initializers.constant(0.))\n",
        "            self.al              = tf.nn.softmax(self.alpha)\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            self.text_embed =   tf.Variable(tf.random.truncated_normal([vocab_size, embed_size // 2], stddev=0.3), name = 'word_embedding')\n",
        "            # self.node_embed = tf.Variable(tf.random.truncated_normal([num_nodes, embed_size // 2], stddev=0.3))\n",
        "            # self.node_embed = tf.clip_by_norm(self.node_embed, clip_norm=1, axes=1)\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.text_emb_lookup = tf.nn.embedding_lookup(self.text_embed, self.text_all)\n",
        "            self.text_feature = tf.reduce_mean(self.text_emb_lookup, axis=1)\n",
        "\n",
        "        self._build_model()\n",
        "\n",
        "\n",
        "    def load_inducing_points(self, inducing_points):\n",
        "        self.inducing_points = tf.compat.v1.assign(self.inducing_points, inducing_points)\n",
        "\n",
        "\n",
        "    def ggp_proces (self, features, z_features):\n",
        "        delta = 0.01\n",
        "        Kxz = self.kernel(features, z_features)\n",
        "        Kzz = self.kernel(z_features, z_features) +tf.eye(self.inducing_num) * delta\n",
        "        Kzz = tf.stop_gradient(Kzz)\n",
        "\n",
        "        if self.trans_order == 3:\n",
        "            Kxz_0 = Kxz\n",
        "            Kxz_1 = sd_matmul(self.trans_mat, Kxz)\n",
        "            Kxz_2 = sd_matmul(self.trans_mat, sd_matmul(self.trans_mat, Kxz))\n",
        "            Kxz_3 = sd_matmul(self.trans_mat, Kxz_2)\n",
        "            Kxz = self.al[0]*Kxz_0 + self.al[1]*Kxz_1 + self.al[2] * Kxz_2 + self.al[3]*Kxz_3\n",
        "\n",
        "        Lz = tf.linalg.cholesky(Kzz)\n",
        "\n",
        "        # Compute the projection matrix A\n",
        "        A = tf.linalg.triangular_solve(Lz, tf.transpose(Kxz), lower=True)\n",
        "        if not self.whiten:\n",
        "            A = tf.linalg.triangular_solve(tf.transpose(Lz), A, lower=False)\n",
        "\n",
        "        # construct the conditional mean\n",
        "        fmean = tf.matmul(A, self.q_mu, transpose_a=True)\n",
        "        return fmean\n",
        "\n",
        "\n",
        "    def compute_loss(self, emb_a, emb_b, emb_n):\n",
        "        positive_loss = tf.reduce_sum(emb_a * emb_b, axis = 1)\n",
        "        negative_loss = -tf.reduce_sum(emb_a * emb_n, axis = 1)\n",
        "        p_likeli = positive_loss - tf.math.softplus(positive_loss)     #log(sigmoid(x))\n",
        "        n_likeli = negative_loss - tf.math.softplus(negative_loss)\n",
        "\n",
        "        total_loss = -tf.reduce_mean(p_likeli + n_likeli)\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    def get_distance(self, feature, ind_points):\n",
        "        feature_prime = tf.expand_dims(feature, axis = 1)\n",
        "        ind_points_prime = tf.expand_dims(ind_points, axis = 0)\n",
        "        dist = tf.reduce_mean(tf.square(feature_prime - ind_points_prime), axis = 2) #[feature_num, induc_num]\n",
        "        return tf.reduce_max(tf.sqrt(dist))\n",
        "\n",
        "\n",
        "    def dmte_embeddings(self, text_features):\n",
        "        first_order_emb = text_features\n",
        "        second_orde_emb = sd_matmul(self.trans_mat, self.text_feature)\n",
        "\n",
        "        return first_order_emb + second_orde_emb\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        self.text_emb = self.dmte_embeddings(self.text_feature)\n",
        "        self.struct_emb = self.ggp_proces(self.text_emb, self.inducing_points)\n",
        "\n",
        "        self.text_emb_a = tf.nn.embedding_lookup(self.text_emb, self.node_a_ids)\n",
        "        self.text_emb_b = tf.nn.embedding_lookup(self.text_emb, self.node_b_ids)\n",
        "        self.text_emb_n = tf.nn.embedding_lookup(self.text_emb, self.node_n_ids)\n",
        "        self.text_loss  = self.compute_loss(self.text_emb_a, self.text_emb_b, self.text_emb_n)\n",
        "\n",
        "        self.struct_emb_a = tf.nn.embedding_lookup(self.struct_emb, self.node_a_ids)\n",
        "        self.struct_emb_b = tf.nn.embedding_lookup(self.struct_emb, self.node_b_ids)\n",
        "        self.struct_emb_n = tf.nn.embedding_lookup(self.struct_emb, self.node_n_ids)\n",
        "        self.struct_loss  = self.compute_loss(self.struct_emb_a, self.struct_emb_b, self.struct_emb_n)\n",
        "\n",
        "        self.total_loss = self.text_loss + 0.3 * self.struct_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4929526b",
      "metadata": {
        "id": "4929526b"
      },
      "source": [
        "# ***CLASSIFY***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "30a95f69",
      "metadata": {
        "id": "30a95f69"
      },
      "outputs": [],
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        # X_train = [self.embeddings[x] for x in X]\n",
        "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
        "\n",
        "        # Access the values of X and Y based on the shuffled indices\n",
        "\n",
        "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "\n",
        "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d9da28",
      "metadata": {
        "id": "f8d9da28"
      },
      "source": [
        "# ***RUN (Single Execution)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db87599",
      "metadata": {
        "id": "3db87599"
      },
      "outputs": [],
      "source": [
        "data = DataLoader(f'{parent_path}/{data_text_file}', f'{parent_path}/{split_graph_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7067392d",
      "metadata": {
        "id": "7067392d"
      },
      "outputs": [],
      "source": [
        "# Saving embeddings\n",
        "#embed_file = f\"{parent_path}/Results/DetGP/embed_link_pred_{split_graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
        "#embed_file = f\"{parent_path}/Results/DetGP/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\" # For node classification the whole graph is used\n",
        "embed_file = f\"{parent_path}/embed_{split_graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6548ff96",
      "metadata": {
        "id": "6548ff96"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "        model = DetGP(data.num_vocab, data.num_nodes, data.text, data.edges)\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "        train_op = opt.minimize(model.total_loss)\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        inducing_points = get_initial_inducing(sess, model, inducing_num)\n",
        "        model.load_inducing_points(inducing_points)\n",
        "\n",
        "        # Training\n",
        "        start_time = datetime.now()\n",
        "        for epoch in range(num_epoch):\n",
        "            batches=data.generate_batches()\n",
        "            num_batch=len(batches)\n",
        "            for i in range(num_batch):\n",
        "                batch=batches[i]\n",
        "\n",
        "                node1, node2, node3=zip(*batch)\n",
        "                node1, node2, node3=np.array(node1),np.array(node2),np.array(node3)\n",
        "                # text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                feed_dict={\n",
        "                    #model.edges: data.edges,\n",
        "                    #model.text_all: data.text,\n",
        "                    #model.inducing_points: inducing_points,\n",
        "                    model.node_a_ids: node1,\n",
        "                    model.node_b_ids: node2,\n",
        "                    model.node_n_ids: node3}\n",
        "\n",
        "                # run the graph\n",
        "                _, loss_batch, al  = sess.run([train_op, model.total_loss, model.al],feed_dict=feed_dict)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        print(f'Total time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "        text_emb, struct_emb = sess.run([model.text_emb_a, model.struct_emb_a], feed_dict = {\n",
        "            # model.edges: data.edges,\n",
        "            # model.text_all: data.text,\n",
        "            model.node_a_ids: np.arange(data.num_nodes)\n",
        "        })\n",
        "\n",
        "        embed = np.concatenate((text_emb, struct_emb), axis=1)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed = embed.tolist()"
      ],
      "metadata": {
        "id": "vfgyU5-o1W6P"
      },
      "id": "vfgyU5-o1W6P",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_with_no_embeddings = []\n",
        "with open(embed_file, 'wb') as f:\n",
        "    for node in range(data.num_nodes):\n",
        "        if embed[node]:\n",
        "            f.write((' '.join(map(str, embed[node])) + '\\n').encode())\n",
        "        else:\n",
        "            nodes_with_no_embeddings.append(node)\n",
        "            f.write('\\n'.encode()) # For link prediction\n",
        "            #f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification"
      ],
      "metadata": {
        "id": "wgR8XHs61UFW"
      },
      "id": "wgR8XHs61UFW",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_with_no_embeddings"
      ],
      "metadata": {
        "id": "lejjXfEg2L8K"
      },
      "id": "lejjXfEg2L8K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "64ed64d9",
      "metadata": {
        "id": "64ed64d9"
      },
      "source": [
        "## Link Prediction (Single and multiple executions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928a7721",
      "metadata": {
        "id": "928a7721"
      },
      "outputs": [],
      "source": [
        "# embed_files contains the node embeddings\n",
        "embed_files = [\n",
        "    f\"{parent_path}/Results/DetGP/embed_link_pred_{graph}_{txtf}.txt\"\n",
        "    for txtf in [\"data-v3-500\",\n",
        "                 \"data-v3-500C\",\n",
        "                 \"YAKE10\",\n",
        "                 \"PosR10\",\n",
        "                 \"PosR5\",\n",
        "                 \"YAKE5\",\n",
        "                 \"RAKE10\",\n",
        "                 \"RAKE5\",\n",
        "                 \"RAKE10C\",\n",
        "                 \"RAKE5C\",\n",
        "                 \"TFIDF5\",\n",
        "                 \"TFIDF10\",\n",
        "                 \"TextR10\",\n",
        "                 \"TextR5\",\n",
        "                 \"TopicR10\",\n",
        "                 \"TopicR5\"]\n",
        "    for graph in split_graph_files\n",
        "]\n",
        "\n",
        "with open(f'{parent_path}/Results/DetGP/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode() != '\\n': #j.decode().strip():\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split()))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, edge.strip().decode().split())) for edge in f]\n",
        "\n",
        "      nodes = list(set([node for edge in edges for node in edge]))\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for node1, node2 in edges:\n",
        "          if node1 in node2vec.keys() and node2 in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[node1], node2vec[node2])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == node2 or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[node1], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/DetGP/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94644c8",
      "metadata": {
        "id": "a94644c8"
      },
      "source": [
        "## Node Classification (Single and multiple executions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embed_files contains the node embedding\n",
        "''' embed_files = [\n",
        "    f\"{parent_path}/Results/DetGP/embed_node_clf_graph_{txtf}.txt\"\n",
        "    for txtf in [\"data-v3-500\",\n",
        "                 \"data-v3-500C\",\n",
        "                 \"YAKE10\",\n",
        "                 \"PosR10\",\n",
        "                 \"PosR5\",\n",
        "                 \"YAKE5\",\n",
        "                 \"RAKE10\",\n",
        "                 \"RAKE5\",\n",
        "                 \"RAKE10C\",\n",
        "                 \"RAKE5C\",\n",
        "                 \"TFIDF5\",\n",
        "                 \"TFIDF10\",\n",
        "                 \"TextR10\",\n",
        "                 \"TextR5\",\n",
        "                 \"TopicR10\",\n",
        "                 \"TopicR5\"]] '''\n",
        "\n",
        "embed_files = ['embed_graph_YAKE.txt']\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label"
      ],
      "metadata": {
        "id": "kalXzjPw2ZXE"
      },
      "id": "kalXzjPw2ZXE",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68729e4",
      "metadata": {
        "id": "d68729e4"
      },
      "outputs": [],
      "source": [
        "for ef in embed_files:\n",
        "  X = []\n",
        "  Y = []\n",
        "  vectors = get_vectors_from_file(ef)\n",
        "\n",
        "  for node in range(len(data.nodes)):\n",
        "    if node in data.nodes:\n",
        "      tag_list = tags[node].strip().split()\n",
        "      # Y.append([(int)(i) for i in tags])\n",
        "      lli = [str(i) for i in tag_list]\n",
        "      if len(lli) > 1 and np.array(vectors[node]).any() != zero_list.any():\n",
        "          X.append(node)\n",
        "          Y.append(lli[1:][0])\n",
        "\n",
        "  # This part of the code uses only the X and Y lists created above\n",
        "  mi = {}\n",
        "  ma = {}\n",
        "  li1 = []\n",
        "  li2 = []\n",
        "\n",
        "  with open(node_clf_results_file, 'a') as f: # f'{parent_path}/Results/DetGP/{node_clf_results_file}'\n",
        "\n",
        "    f.write(f\"{ef.split('/')[-1]} \\n\")\n",
        "    print(ef.split('/')[-1])\n",
        "\n",
        "    for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "      for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "        clf = Classifier(vectors=vectors, # All node embeddings\n",
        "                        clf=LogisticRegression(max_iter=10000))\n",
        "\n",
        "        # Ensure X and Y are not empty before splitting and training\n",
        "        if X and Y:\n",
        "          result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "          # Results\n",
        "          li1.append(result['micro'])\n",
        "          li2.append(result['macro'])\n",
        "        else:\n",
        "          print(f\"Skipping classification for ratio {clf_ratio[i]} due to empty training data\")\n",
        "          # Handle the case where X or Y is empty, perhaps by appending a placeholder or skipping the loop\n",
        "          continue\n",
        "\n",
        "      if li1 and li2: # Only calculate mean if there were valid results\n",
        "          mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "          ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "          print(mi)\n",
        "          print(ma)\n",
        "          print()\n",
        "\n",
        "          f.writelines(str(str(mi)+str(ma)))\n",
        "          f.write('\\n')\n",
        "\n",
        "      # Reinitialize the dictionaries and lists for the next ratio\n",
        "      mi = {}\n",
        "      ma = {}\n",
        "      li1 = []\n",
        "      li2 = []\n",
        "\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***RUN (Multiple executions)***"
      ],
      "metadata": {
        "id": "Tu3OmRGbvGk3"
      },
      "id": "Tu3OmRGbvGk3"
    },
    {
      "cell_type": "code",
      "source": [
        "for gf in split_graph_files: # for gf in split_graph_files: for link prediction. For node classification just use for gf in ['graph.txt']:\n",
        "    for t, txtf in enumerate(data_text_files):\n",
        "      MAX_LEN = MAX_LENS[t]\n",
        "      print(f'The maximum length is: {MAX_LEN}')\n",
        "\n",
        "      data = DataLoader(f'{parent_path}/{data_text_file}', f'{parent_path}/{split_graph_file}', text_filename=txtf)\n",
        "\n",
        "      # Logging the execution details\n",
        "      with open(f'{parent_path}/Results/DetGP/{log_file}', 'a') as f:\n",
        "          f.write(f'Processing graph: {gf}, text: {txtf}\\n')\n",
        "\n",
        "      print(f'Processing graph: {gf}, text: {txtf}')\n",
        "\n",
        "      with tf.Graph().as_default():\n",
        "        sess = tf.compat.v1.Session()\n",
        "        with sess.as_default():\n",
        "            model = DetGP(data.num_vocab, data.num_nodes, data.text, data.edges)\n",
        "            opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "            train_op = opt.minimize(model.total_loss)\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "            inducing_points = get_initial_inducing(sess, model, inducing_num)\n",
        "            model.load_inducing_points(inducing_points)\n",
        "\n",
        "            # Training\n",
        "            start_time = datetime.now()\n",
        "            for epoch in range(num_epoch):\n",
        "                batches=data.generate_batches()\n",
        "                num_batch=len(batches)\n",
        "                for i in range(num_batch):\n",
        "                    batch=batches[i]\n",
        "\n",
        "                    node1, node2, node3=zip(*batch)\n",
        "                    node1, node2, node3=np.array(node1),np.array(node2),np.array(node3)\n",
        "                    # text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                    feed_dict={\n",
        "                        #model.edges: data.edges,\n",
        "                        #model.text_all: data.text,\n",
        "                        #model.inducing_points: inducing_points,\n",
        "                        model.node_a_ids: node1,\n",
        "                        model.node_b_ids: node2,\n",
        "                        model.node_n_ids: node3}\n",
        "\n",
        "                    # run the graph\n",
        "                    _, loss_batch, al  = sess.run([train_op, model.total_loss, model.al],feed_dict=feed_dict)\n",
        "\n",
        "            end_time = datetime.now()\n",
        "            with open(f'{parent_path}/Results/DetGP/{log_file}', 'a') as f:\n",
        "              f.write(f'Time: {((end_time - start_time).total_seconds()) / 60.0} min\\n')\n",
        "\n",
        "            print(f'Total time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "            text_emb, struct_emb = sess.run([model.text_emb_a, model.struct_emb_a], feed_dict = {\n",
        "                # model.edges: data.edges,\n",
        "                # model.text_all: data.text,\n",
        "                model.node_a_ids: np.arange(data.num_nodes)\n",
        "            })\n",
        "            embed = np.concatenate((text_emb, struct_emb), axis=1)\n",
        "            embed = embed.tolist()\n",
        "\n",
        "            # Saving embeddings\n",
        "            embed_file = f\"{parent_path}/Results/DetGP/embed_link_pred_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "            #embed_file = f\"{parent_path}/Results/DetGP/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "\n",
        "            with open(embed_file, 'wb') as f:\n",
        "              for node in range(data.num_nodes):\n",
        "                  if embed[node]:\n",
        "                      f.write((' '.join(map(str, embed[node])) + '\\n').encode())\n",
        "                  else:\n",
        "                      nodes_with_no_embeddings.append(node)\n",
        "                      f.write('\\n'.encode()) # For link prediction\n",
        "                      #f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
        "\n",
        "      gc.collect()"
      ],
      "metadata": {
        "id": "nrHTGwK0vMb1"
      },
      "id": "nrHTGwK0vMb1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}