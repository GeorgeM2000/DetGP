{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/DetGP/blob/master/DetGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa86248",
      "metadata": {
        "id": "dfa86248"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "89ff0f1c",
      "metadata": {
        "id": "89ff0f1c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import networkx as nx\n",
        "import tensorflow as tf\n",
        "import scipy.cluster\n",
        "import gc\n",
        "import math\n",
        "import shutil\n",
        "import re\n",
        "import os\n",
        "\n",
        "from tensorflow.sparse import sparse_dense_matmul as sd_matmul\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9757d0b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9757d0b2",
        "outputId": "8c352234-8fa9-4126-d5ee-86b89cc51945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "GPU is ready for use!\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Optional: Set GPU memory growth to avoid over-allocation\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU is ready for use!\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3536e3d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3536e3d1",
        "outputId": "660ea1d0-cf95-424b-e164-ca04eb9f5323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is TensorFlow using GPU? True\n",
            "GPU device: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "print(\"Is TensorFlow using GPU?\", tf.test.is_built_with_cuda())  # Should return True\n",
        "print(\"GPU device:\", tf.test.gpu_device_name())  # Should return something like /device:GPU:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "40f05ae0",
      "metadata": {
        "id": "40f05ae0"
      },
      "outputs": [],
      "source": [
        "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuTWRkRZI-T-",
        "outputId": "09de6a49-ce67-482d-fb36-eb638f7b4732"
      },
      "id": "nuTWRkRZI-T-",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5149d36a",
      "metadata": {
        "id": "5149d36a"
      },
      "source": [
        "# ***Variables & General Functionality***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "35ed8cbb",
      "metadata": {
        "id": "35ed8cbb"
      },
      "outputs": [],
      "source": [
        "dataset_name    = \"arxiv\"\n",
        "parent_path     = f'Datasets/{dataset_name}/graph-v2'\n",
        "parent_path     = '/content'\n",
        "\n",
        "data_text_files = [\"data-v3-500.txt\", \"data-v3-500C.txt\", \"YAKE10.txt\", \"YAKE5.txt\", \"RAKE10.txt\", \"RAKE5.txt\", \"RAKE10C.txt\", \"RAKE5C.txt\", \"TFIDF10.txt\", \"TFIDF5.txt\", \"PosR5.txt\",\n",
        "                   \"PosR10.txt\", \"TextR5.txt\", \"TextR10.txt\", \"TopicR5.txt\", \"TopicR10.txt\"]\n",
        "\n",
        "graph_file             = 'graph.txt' # For node classification\n",
        "categories_file        = 'group-v3.txt'\n",
        "data_text_file         = \"data.txt\" # For single execution\n",
        "\n",
        "log_file               = 'DetGP_Execution_Logs.txt'\n",
        "link_pred_results_file = 'DetGP_Link_Pred_Res.txt'\n",
        "node_clf_results_file  = 'DetGP_Node_Clf_Res.txt'\n",
        "\n",
        "\n",
        "split_graph_file  = 'sgraph15.txt' # For single execution\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "\n",
        "test_graph_file   = 'tgraph85.txt' # For single execution\n",
        "test_graph_files  = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "75273cbc",
      "metadata": {
        "id": "75273cbc"
      },
      "outputs": [],
      "source": [
        "# Parameters for node classification\n",
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num = 5\n",
        "train_classifier = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "94b6de36",
      "metadata": {
        "id": "94b6de36"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 300\n",
        "MAX_LENS = []\n",
        "neg_table_size = 1000000\n",
        "NEG_SAMPLE_POWER = 0.75\n",
        "batch_size = 64 # Orig: 128\n",
        "num_epoch = 50\n",
        "embed_size = 200\n",
        "lr = 1e-3\n",
        "inducing_num = 20\n",
        "report_epoch_num = 1\n",
        "eval_epoch_num = 10 * report_epoch_num\n",
        "random_seed = 42\n",
        "\n",
        "GPU_USAGE = 0.5\n",
        "GPU_ID = 0\n",
        "\n",
        "encoder_type = 'wavg'  # 'dmte' 'wavg'\n",
        "kernel_type = 'linear'\n",
        "model_name = 'DetGP'\n",
        "trans_order = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "40ceccff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ceccff",
        "outputId": "bccba77b-5d99-4dda-c3e0-2e7997ddbfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== data.txt ===\n",
            "Mean word count: 91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find the average number of words from each data text file\n",
        "for txtf in ['data.txt']: # 1) ['data.txt'] 2) data_text_files:\n",
        "    total_word_count = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            total_word_count += len(re.findall(r\"\\b\\w+\\b\", line))\n",
        "            total_lines += 1\n",
        "\n",
        "    mean_word_count = total_word_count / total_lines if total_lines > 0 else 0\n",
        "    MAX_LENS.append(int(math.ceil(mean_word_count)))\n",
        "    print(f'=== {txtf} ===')\n",
        "    print(\"Mean word count:\", math.ceil(mean_word_count))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b1dac2b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1dac2b8",
        "outputId": "feb15f31-719a-478d-d13d-894b60102dcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "MAX_LENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4b68be1f",
      "metadata": {
        "id": "4b68be1f"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For single execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8d44f8b8",
      "metadata": {
        "id": "8d44f8b8"
      },
      "outputs": [],
      "source": [
        "zero_list = []\n",
        "for i in range(0, embed_size):\n",
        "    zero_list.append(0)\n",
        "zero_list = np.array(zero_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "80f8ed04",
      "metadata": {
        "id": "80f8ed04"
      },
      "outputs": [],
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "\n",
        "  with open(f'{file_path}', \"r\") as f:\n",
        "      for idx, line in enumerate(f):\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7775a009",
      "metadata": {
        "id": "7775a009"
      },
      "source": [
        "# ***Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "48de3cf2",
      "metadata": {
        "id": "48de3cf2"
      },
      "outputs": [],
      "source": [
        "def edges_to_undirect(node_num, edges):\n",
        "    # When given edge [i,j] build undirect graph with edge [i,j] and [j,i], each edge only appear once\n",
        "    # Create unique hash codes for the nodes in the edges\n",
        "    edge_hash = []\n",
        "    for edge in edges:\n",
        "        edge_hash.append(edge[0]*node_num+edge[1])\n",
        "        edge_hash.append(edge[1]*node_num+edge[0])\n",
        "\n",
        "    edge_hash = list(set(edge_hash)) # edge_hash will have unique hash codes for the edges\n",
        "    ud_edges = [[v // node_num, v % node_num] for v in edge_hash]\n",
        "    return ud_edges\n",
        "\n",
        "def get_initial_inducing(sess, model, inducing_num):\n",
        "    text_feature = sess.run(model.text_feature)\n",
        "    inducing_points = scipy.cluster.vq.kmeans2(text_feature, inducing_num, minit='points')[0]\n",
        "    return inducing_points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b2c562",
      "metadata": {
        "id": "42b2c562"
      },
      "source": [
        "# ***Negative Sample***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b32b8757",
      "metadata": {
        "id": "b32b8757"
      },
      "outputs": [],
      "source": [
        "def InitNegTable(edges):\n",
        "    a_list, b_list = zip(*edges)\n",
        "    a_list = list(a_list)\n",
        "    b_list = list(b_list)\n",
        "    node = a_list\n",
        "    node.extend(b_list)\n",
        "\n",
        "    node_degree = {}\n",
        "    for i in node:\n",
        "        if i in node_degree:\n",
        "            node_degree[i] += 1\n",
        "        else:\n",
        "            node_degree[i] = 1\n",
        "    sum_degree = 0\n",
        "    for i in node_degree.values():\n",
        "        sum_degree += pow(i, 0.75)\n",
        "\n",
        "    por = 0\n",
        "    cur_sum = 0\n",
        "    vid = -1\n",
        "    neg_table = []\n",
        "    degree_list = list(node_degree.values())\n",
        "    node_id = list(node_degree.keys())\n",
        "    for i in range(neg_table_size):\n",
        "        if ((i + 1) / float(neg_table_size)) > por:\n",
        "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
        "            por = cur_sum / sum_degree\n",
        "            vid += 1\n",
        "        neg_table.append(node_id[vid])\n",
        "\n",
        "    return neg_table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1daf22ca",
      "metadata": {
        "id": "1daf22ca"
      },
      "source": [
        "# ***Dataloader***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5e1b26dc",
      "metadata": {
        "id": "5e1b26dc"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, text_path, graph_path, edge_split_ratio=None):\n",
        "        #self.graph, self.train_graph, self.test_graph = self.load_graph(graph_path, edge_split_ratio)\n",
        "        text_file, graph_file = self.load(text_path, graph_path)\n",
        "        self.nodes = []\n",
        "        self.edges = self.load_edges(graph_file)\n",
        "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "        #self.train_edges = edges_to_undirect(self.num_nodes, self.train_graph.edges())\n",
        "        #self.test_edges = edges_to_undirect(self.num_nodes, self.test_graph.edges())\n",
        "        self.negative_table = InitNegTable(self.edges)\n",
        "\n",
        "    def load(self, text_path, graph_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        for a in range(0, len(text_file)):\n",
        "            text_file[a] = str(text_file[a])\n",
        "\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "\n",
        "        return text_file, graph_file\n",
        "\n",
        "    def load_edges(self, graph_file):\n",
        "        edges = []\n",
        "        for i in graph_file:\n",
        "            edges.append(list(map(int, i.strip().decode().split())))\n",
        "\n",
        "        for ll in edges:\n",
        "            for ed in ll:\n",
        "                if ed not in self.nodes:\n",
        "                    self.nodes.append(ed)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "        print(\"Total load %d edges.\" % len(edges))\n",
        "        return edges\n",
        "\n",
        "    def get_adj_list(self, edges, node_num):\n",
        "        adj_list = []\n",
        "        for node in range(node_num):\n",
        "                nbr = [node]\n",
        "                for edge in edges:\n",
        "                        if node in edge:\n",
        "                                nbr += edge\n",
        "                adj_list.append(list(set(nbr)))\n",
        "        return adj_list\n",
        "\n",
        "\n",
        "    def load_graph(self, graph_path, edge_split_ratio):\n",
        "        total_graph = nx.Graph()\n",
        "        train_graph = nx.Graph()\n",
        "        test_graph = nx.Graph()\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "        for line in graph_file:\n",
        "            edge = list(map(int, line.strip().decode().split()))\n",
        "            total_graph.add_edge(edge[0],edge[1])\n",
        "\n",
        "            if np.random.uniform(0.0, 1.0) <= edge_split_ratio:\n",
        "                train_graph.add_edge(edge[0],edge[1])\n",
        "            else:\n",
        "                test_graph.add_edge(edge[0],edge[1])\n",
        "\n",
        "        return total_graph, train_graph, test_graph\n",
        "\n",
        "    # Original load_text() method\n",
        "    \"\"\" def load_text(self, text_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        vocab = learn.preprocessing.VocabularyProcessor(MAX_LEN)\n",
        "        text = np.array(list(vocab.fit_transform(text_file)))\n",
        "        num_vocab = len(vocab.vocabulary_)\n",
        "        num_nodes = len(text_file)\n",
        "        return text, num_vocab, num_nodes \"\"\"\n",
        "\n",
        "    def load_text(self, text_file):\n",
        "        #text_file = open(text_path, 'rb').readlines()\n",
        "        #for a in range(0, len(text_file)): text_file[a] = str(text_file[a])\n",
        "\n",
        "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=None,  # Set a limit if needed\n",
        "            output_mode='int',\n",
        "            output_sequence_length=MAX_LEN\n",
        "        )\n",
        "\n",
        "        text_data = [line.strip() for line in text_file]\n",
        "        vectorize_layer.adapt(text_data)\n",
        "        text = vectorize_layer(text_data).numpy()\n",
        "        return text, len(vectorize_layer.get_vocabulary()), len(text)\n",
        "\n",
        "\n",
        "    def subgraph_edges(self, node_list):\n",
        "        subg_edges = self.train_graph.subgraph(node_list).edges()\n",
        "        return edges_to_undirect(self.num_nodes, subg_edges)\n",
        "\n",
        "    # Original negative_sampling() method\n",
        "    \"\"\" def negative_sampling(self, graph, edges):\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "        #np.random.seed(config.random_seed)\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = random.choice(list(graph.nodes))\n",
        "            while neg_node in graph.neighbors(node1[i]):\n",
        "                neg_node = random.choice(list(graph.nodes))\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "        return sample_edges \"\"\"\n",
        "\n",
        "    def negative_sampling(self, edges):\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)]\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = func()\n",
        "            while node1[i] == neg_node or node2[i] == neg_node:\n",
        "                neg_node = func()\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "\n",
        "        return sample_edges\n",
        "\n",
        "\n",
        "    def generate_batches(self, mode=None):\n",
        "        num_batch = len(self.edges) // batch_size\n",
        "        edges = self.edges\n",
        "\n",
        "        if mode == 'add':\n",
        "          num_batch += 1\n",
        "          edges.extend(edges[:(batch_size - len(self.edges) // batch_size)])\n",
        "\n",
        "        if mode != 'add':\n",
        "          random.shuffle(edges)\n",
        "\n",
        "        sample_edges = edges[:num_batch*batch_size]\n",
        "        sample_edges = self.negative_sampling(sample_edges)\n",
        "\n",
        "        batches = [sample_edges[i * batch_size:(i + 1) * batch_size] for i in range(num_batch)]\n",
        "        return batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af19bc1c",
      "metadata": {
        "id": "af19bc1c"
      },
      "source": [
        "# ***Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2bd3e22d",
      "metadata": {
        "id": "2bd3e22d"
      },
      "outputs": [],
      "source": [
        "def RBF_Kernel(feature_a, feature_b, gamma = 1000.):\n",
        "    f_a = tf.expand_dims(feature_a, 1)  #[node_num_a, 1, f_dim]\n",
        "    f_b = tf.expand_dims(feature_b, 0)  #[1, node_num_b, f_dim]\n",
        "    k_ab = tf.reduce_sum(tf.square(f_a-f_b), axis = 2)\n",
        "    return tf.exp(-k_ab / gamma)\n",
        "\n",
        "def Linear_Kernel(feature_a, feature_b):\n",
        "    k_ab = tf.matmul(feature_a, feature_b, transpose_b = True)\n",
        "    return k_ab + 1.\n",
        "\n",
        "\n",
        "def Polynomial_Kernel(feature_a, feature_b):\n",
        "    return Linear_Kernel(feature_a, feature_b)**2.\n",
        "\n",
        "\n",
        "def regularized_adj(adj):\n",
        "    return adj/tf.sparse.reduce_sum(adj, axis = 1, keepdims = True)\n",
        "\n",
        "\n",
        "def get_transition_matrix(Adj): # a sparse adjacency matrix\n",
        "    reg_Adj = tf.sparse.add(Adj, tf.sparse.eye(tf.shape(Adj)[0]))\n",
        "    return reg_Adj/tf.sparse.reduce_sum(reg_Adj,axis = 1, keepdims = True)\n",
        "\n",
        "\n",
        "def norm_trans(adj_mat):\n",
        "    return adj_mat / tf.sparse.reduce_sum(adj_mat, axis = 1, keepdims = True)\n",
        "\n",
        "\n",
        "class DetGP():\n",
        "    def __init__(self, vocab_size, num_nodes, text_data, edges):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.kernel = Linear_Kernel\n",
        "        self.inducing_num = inducing_num\n",
        "        self.whiten = False\n",
        "        self.embedding_dim = embed_size // 2\n",
        "        self.trans_order = trans_order\n",
        "\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "            self.text_all = tf.constant(text_data, dtype = tf.int32)\n",
        "            self.node_a_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'a_ids')\n",
        "            self.node_b_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'b_ids')\n",
        "            self.node_n_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'n_ids')\n",
        "\n",
        "            self.edges = tf.constant(edges, dtype=tf.int64)\n",
        "            self.adj_mat = tf.sparse.SparseTensor(\n",
        "                            self.edges, tf.ones(tf.shape(self.edges)[0]),\n",
        "                            dense_shape=[self.num_nodes, self.num_nodes])\n",
        "            self.trans_mat = get_transition_matrix(self.adj_mat)\n",
        "            #self.initial_inducing_points = tf.placeholder(tf.float32, [None, embed_size/2], name = 'inducing')\n",
        "\n",
        "        with tf.compat.v1.variable_scope('ggp') as scope:\n",
        "            self.inducing_points = tf.compat.v1.get_variable(name = 'inducing_points', shape = [self.inducing_num, self.embedding_dim])\n",
        "            self.q_mu            = tf.compat.v1.get_variable(name = 'embedding_mu', shape = [self.inducing_num, self.embedding_dim], initializer = tf.initializers.constant(0.1))\n",
        "            self.alpha           = tf.compat.v1.get_variable(name = 'alpha', shape = [trans_order + 1], initializer = tf.initializers.constant(0.))\n",
        "            self.al              = tf.nn.softmax(self.alpha)\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            self.text_embed =   tf.Variable(tf.random.truncated_normal([vocab_size, embed_size // 2], stddev=0.3), name = 'word_embedding')\n",
        "            # self.node_embed = tf.Variable(tf.random.truncated_normal([num_nodes, embed_size // 2], stddev=0.3))\n",
        "            # self.node_embed = tf.clip_by_norm(self.node_embed, clip_norm=1, axes=1)\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.text_emb_lookup = tf.nn.embedding_lookup(self.text_embed, self.text_all)\n",
        "            self.text_feature = tf.reduce_mean(self.text_emb_lookup, axis=1)\n",
        "\n",
        "        self._build_model()\n",
        "\n",
        "\n",
        "    def load_inducing_points(self, inducing_points):\n",
        "        self.inducing_points = tf.compat.v1.assign(self.inducing_points, inducing_points)\n",
        "\n",
        "\n",
        "    def ggp_proces (self, features, z_features):\n",
        "        delta = 0.01\n",
        "        Kxz = self.kernel(features, z_features)\n",
        "        Kzz = self.kernel(z_features, z_features) +tf.eye(self.inducing_num) * delta\n",
        "        Kzz = tf.stop_gradient(Kzz)\n",
        "\n",
        "        if self.trans_order == 3:\n",
        "            Kxz_0 = Kxz\n",
        "            Kxz_1 = sd_matmul(self.trans_mat, Kxz)\n",
        "            Kxz_2 = sd_matmul(self.trans_mat, sd_matmul(self.trans_mat, Kxz))\n",
        "            Kxz_3 = sd_matmul(self.trans_mat, Kxz_2)\n",
        "            Kxz = self.al[0]*Kxz_0 + self.al[1]*Kxz_1 + self.al[2] * Kxz_2 + self.al[3]*Kxz_3\n",
        "\n",
        "        Lz = tf.linalg.cholesky(Kzz)\n",
        "\n",
        "        # Compute the projection matrix A\n",
        "        A = tf.linalg.triangular_solve(Lz, tf.transpose(Kxz), lower=True)\n",
        "        if not self.whiten:\n",
        "            A = tf.linalg.triangular_solve(tf.transpose(Lz), A, lower=False)\n",
        "\n",
        "        # construct the conditional mean\n",
        "        fmean = tf.matmul(A, self.q_mu, transpose_a=True)\n",
        "        return fmean\n",
        "\n",
        "\n",
        "    def compute_loss(self, emb_a, emb_b, emb_n):\n",
        "        positive_loss = tf.reduce_sum(emb_a * emb_b, axis = 1)\n",
        "        negative_loss = -tf.reduce_sum(emb_a * emb_n, axis = 1)\n",
        "        p_likeli = positive_loss - tf.math.softplus(positive_loss)     #log(sigmoid(x))\n",
        "        n_likeli = negative_loss - tf.math.softplus(negative_loss)\n",
        "\n",
        "        total_loss = -tf.reduce_mean(p_likeli + n_likeli)\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    def get_distance(self, feature, ind_points):\n",
        "        feature_prime = tf.expand_dims(feature, axis = 1)\n",
        "        ind_points_prime = tf.expand_dims(ind_points, axis = 0)\n",
        "        dist = tf.reduce_mean(tf.square(feature_prime - ind_points_prime), axis = 2) #[feature_num, induc_num]\n",
        "        return tf.reduce_max(tf.sqrt(dist))\n",
        "\n",
        "\n",
        "    def dmte_embeddings(self, text_features):\n",
        "        first_order_emb = text_features\n",
        "        second_orde_emb = sd_matmul(self.trans_mat, self.text_feature)\n",
        "\n",
        "        return first_order_emb + second_orde_emb\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        self.text_emb = self.dmte_embeddings(self.text_feature)\n",
        "        self.struct_emb = self.ggp_proces(self.text_emb, self.inducing_points)\n",
        "\n",
        "        self.text_emb_a = tf.nn.embedding_lookup(self.text_emb, self.node_a_ids)\n",
        "        self.text_emb_b = tf.nn.embedding_lookup(self.text_emb, self.node_b_ids)\n",
        "        self.text_emb_n = tf.nn.embedding_lookup(self.text_emb, self.node_n_ids)\n",
        "        self.text_loss  = self.compute_loss(self.text_emb_a, self.text_emb_b, self.text_emb_n)\n",
        "\n",
        "        self.struct_emb_a = tf.nn.embedding_lookup(self.struct_emb, self.node_a_ids)\n",
        "        self.struct_emb_b = tf.nn.embedding_lookup(self.struct_emb, self.node_b_ids)\n",
        "        self.struct_emb_n = tf.nn.embedding_lookup(self.struct_emb, self.node_n_ids)\n",
        "        self.struct_loss  = self.compute_loss(self.struct_emb_a, self.struct_emb_b, self.struct_emb_n)\n",
        "\n",
        "        self.total_loss = self.text_loss + 0.3 * self.struct_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4929526b",
      "metadata": {
        "id": "4929526b"
      },
      "source": [
        "# ***Classify***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "30a95f69",
      "metadata": {
        "id": "30a95f69"
      },
      "outputs": [],
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        # X_train = [self.embeddings[x] for x in X]\n",
        "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
        "\n",
        "        # Access the values of X and Y based on the shuffled indices\n",
        "\n",
        "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "\n",
        "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train_vec, test_vec, train_y, test_y, classifierStr='SVM', normalize=0):\n",
        "\n",
        "    if classifierStr == 'KNN':\n",
        "        print('Training NN classifier...')\n",
        "        classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "    else:\n",
        "        print('Training SVM classifier...')\n",
        "\n",
        "        classifier = LinearSVC()\n",
        "\n",
        "    if(normalize == 1):\n",
        "        print('Normalize data')\n",
        "        allvec = list(train_vec)\n",
        "        allvec.extend(test_vec)\n",
        "        allvec_normalized = preprocessing.normalize(allvec, norm='l2', axis=1)\n",
        "        train_vec = allvec_normalized[0:len(train_y)]\n",
        "        test_vec = allvec_normalized[len(train_y):]\n",
        "\n",
        "\n",
        "    classifier.fit(train_vec, train_y)\n",
        "    y_pred = classifier.predict(test_vec)\n",
        "    cm = confusion_matrix(test_y, y_pred)\n",
        "\n",
        "    #print(cm)\n",
        "    acc = accuracy_score(test_y, y_pred)\n",
        "    #print(acc)\n",
        "\n",
        "    #macro_f1 = f1_score(test_y, y_pred,pos_label=None, average='macro')\n",
        "    #micro_f1 = f1_score(test_y, y_pred,pos_label=None, average='micro')\n",
        "\n",
        "    macro_f1 = f1_score(test_y, y_pred,pos_label=None, average='macro')\n",
        "    micro_f1 = f1_score(test_y, y_pred,pos_label=None, average='micro')\n",
        "\n",
        "    per = len(train_y) * 1.0 /(len(test_y)+len(train_y))\n",
        "    #print('Classification method:'+classifierStr+'(train, test, Training_Percent): (%d, %d, %f)' % (len(train_y),len(test_y), per ))\n",
        "    print('Classification Accuracy=%f, macro_f1=%f, micro_f1=%f' % (acc, macro_f1, micro_f1))\n",
        "    #print(metrics.classification_report(test_y, y_pred))\n",
        "\n",
        "    return acc, macro_f1, micro_f1"
      ],
      "metadata": {
        "id": "kpvkxUJ-UdXw"
      },
      "id": "kpvkxUJ-UdXw",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f8d9da28",
      "metadata": {
        "id": "f8d9da28"
      },
      "source": [
        "# ***Run (Single Execution)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "144c7143",
      "metadata": {
        "id": "144c7143"
      },
      "outputs": [],
      "source": [
        "label_dic = {}\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  labels = f.readlines()\n",
        "\n",
        "for la in labels:\n",
        "  label_dic[la.split()[0]] = la.split()[1:][0] # la.split()[0] = the node id ----- la.split()[1:][0] = The label of that node. If a node has many labels, take the first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3db87599",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3db87599",
        "outputId": "a8ec88af-c360-418d-d009-74e006221f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total load 5214 edges.\n"
          ]
        }
      ],
      "source": [
        "data = DataLoader(f'{parent_path}/{data_text_file}', f'{parent_path}/{graph_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7067392d",
      "metadata": {
        "id": "7067392d"
      },
      "outputs": [],
      "source": [
        "# Saving embeddings\n",
        "#embed_file = f\"{parent_path}/Results/DetGP/embed_link_pred_{split_graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
        "#embed_file = f\"{parent_path}/Results/DetGP/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\" # For node classification the whole graph ('graph.txt') is used\n",
        "embed_file = f\"{parent_path}/embed_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6548ff96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6548ff96",
        "outputId": "cd620748-de92-4fd8-94b6-324913def565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time: 0.5966267 min\n",
            "Training SVM classifier...\n",
            "Classification Accuracy=0.845614, macro_f1=0.834955, micro_f1=0.845614\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "        model = DetGP(data.num_vocab, data.num_nodes, data.text, data.edges)\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "        train_op = opt.minimize(model.total_loss)\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        inducing_points = get_initial_inducing(sess, model, inducing_num)\n",
        "        model.load_inducing_points(inducing_points)\n",
        "\n",
        "        # Training\n",
        "        start_time = datetime.now()\n",
        "        for epoch in range(num_epoch):\n",
        "            loss_epoch=0\n",
        "            batches=data.generate_batches()\n",
        "            num_batch=len(batches)\n",
        "            for i in range(num_batch):\n",
        "                batch=batches[i]\n",
        "\n",
        "                node1, node2, node3=zip(*batch)\n",
        "                node1, node2, node3=np.array(node1),np.array(node2),np.array(node3)\n",
        "                # text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "                feed_dict={\n",
        "                    #model.edges: data.edges,\n",
        "                    #model.text_all: data.text,\n",
        "                    #model.inducing_points: inducing_points,\n",
        "                    model.node_a_ids: node1,\n",
        "                    model.node_b_ids: node2,\n",
        "                    model.node_n_ids: node3}\n",
        "\n",
        "                # run the graph\n",
        "                _, loss_batch, al  = sess.run([train_op, model.total_loss, model.al],feed_dict=feed_dict)\n",
        "                loss_epoch += loss_batch\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        print(f'Total time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "        text_emb, struct_emb = sess.run([model.text_emb_a, model.struct_emb_a], feed_dict = {\n",
        "            # model.edges: data.edges,\n",
        "            # model.text_all: data.text,\n",
        "            model.node_a_ids: np.arange(data.num_nodes)\n",
        "        })\n",
        "\n",
        "        embed = np.concatenate((text_emb, struct_emb), axis=1)\n",
        "\n",
        "        labels = {}\n",
        "        with open(f'{parent_path}/{categories_file}', 'rb') as f:\n",
        "            for i, j in enumerate(f):\n",
        "                if j != '\\n':\n",
        "                    labels[i] = list(map(int, j.strip().split()))[1]\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "        for i in labels.keys():\n",
        "            X.append(embed[i])\n",
        "            Y.append(labels[i])\n",
        "\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "        train_X, test_X = train_test_split(X, train_size = 0.75, random_state = random_seed)\n",
        "        train_Y, test_Y = train_test_split(Y, train_size = 0.75, random_state = random_seed)\n",
        "\n",
        "        acc, macro_f1, micro_f1 = evaluate(train_X, test_X, train_Y, test_Y, 'SVM', 0)\n",
        "\n",
        "        ''' embed = embed.tolist()\n",
        "        with open(embed_file, 'wb') as f:\n",
        "            for i in range(data.num_nodes):\n",
        "                if embed[i]:\n",
        "                    f.write((' '.join(map(str, embed[i])) + '\\n').encode())\n",
        "                else:\n",
        "                    #f.write('\\n'.encode()) # For link prediction\n",
        "                    f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification '''\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ed64d9",
      "metadata": {
        "id": "64ed64d9"
      },
      "source": [
        "## Link Prediction (Single and multiple executions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928a7721",
      "metadata": {
        "id": "928a7721"
      },
      "outputs": [],
      "source": [
        "# embed_files are the node embedding files for the to-publish results\n",
        "embed_files = [\n",
        "    f\"{parent_path}/Results/DetGP/embed_link_pred_{graph}_{txtf}.txt\"\n",
        "    for txtf in [\"data-v3-500\", \"data-v3-500C\", \"YAKE10\", \"PosR10\", \"PosR5\", \"YAKE5\", \"RAKE10\", \"RAKE5\", \"RAKE10C\", \"RAKE5C\", \"TFIDF5\", \"TFIDF10\", \"TextR10\", \"TextR5\", \"TopicR10\", \"TopicR5\"]\n",
        "    for graph in split_graph_files\n",
        "]\n",
        "\n",
        "with open(f'{parent_path}/Results/DetGP/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode() != '\\n': #j.decode().strip():\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split()))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
        "\n",
        "      nodes = list(set([i for j in edges for i in j]))\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for i, j in edges:\n",
        "          if i in node2vec.keys() and j in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == j or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/DetGP/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94644c8",
      "metadata": {
        "id": "a94644c8"
      },
      "source": [
        "## Node Classification (Single and multiple executions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "d68729e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d68729e4",
        "outputId": "13f64e03-b373-4923-9d35-f0d9e9aab339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed_graph_data.txt\n",
            "Skipping classification for ratio 0.15 due to empty training data.\n",
            "Skipping classification for ratio 0.15 due to empty training data.\n",
            "Skipping classification for ratio 0.15 due to empty training data.\n",
            "Skipping classification for ratio 0.15 due to empty training data.\n",
            "Skipping classification for ratio 0.15 due to empty training data.\n",
            "Skipping classification for ratio 0.45 due to empty training data.\n",
            "Skipping classification for ratio 0.45 due to empty training data.\n",
            "Skipping classification for ratio 0.45 due to empty training data.\n",
            "Skipping classification for ratio 0.45 due to empty training data.\n",
            "Skipping classification for ratio 0.45 due to empty training data.\n",
            "Skipping classification for ratio 0.75 due to empty training data.\n",
            "Skipping classification for ratio 0.75 due to empty training data.\n",
            "Skipping classification for ratio 0.75 due to empty training data.\n",
            "Skipping classification for ratio 0.75 due to empty training data.\n",
            "Skipping classification for ratio 0.75 due to empty training data.\n"
          ]
        }
      ],
      "source": [
        "# embed_files are the node embedding files for the to-publish results\n",
        "embed_files = [\n",
        "    f\"{parent_path}/Results/Net2Net-NE/embed_node_clf_graph_{txtf}.txt\"\n",
        "    for txtf in [\"data-v3-500\", \"data-v3-500C\", \"YAKE10\", \"PosR10\", \"PosR5\", \"YAKE5\", \"RAKE10\", \"RAKE5\", \"RAKE10C\", \"RAKE5C\", \"TFIDF5\", \"TFIDF10\", \"TextR10\", \"TextR5\", \"TopicR10\", \"TopicR5\"]\n",
        "]\n",
        "embed_files = ['embed_graph_data.txt']\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = data.num_nodes # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "\n",
        "  for ef in embed_files:\n",
        "    X = []\n",
        "    Y = []\n",
        "    new_vector = get_vectors_from_file(ef)\n",
        "\n",
        "    for jk in range(0, clf_test_len):\n",
        "      if str(jk) in data.nodes: # If the index \"jk\" is a node\n",
        "        tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "        # Y.append([(int)(i) for i in tags])\n",
        "        lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "        # Check if there are labels and the embedding is not all zeros\n",
        "        if len(lli) > 1 and np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "            X.append(jk)\n",
        "            Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "    # This part of the code uses only the X and Y lists created above\n",
        "    mi = {}\n",
        "    ma = {}\n",
        "    li1 = []\n",
        "    li2 = []\n",
        "    # f'{parent_path}/Results/DetGP/{node_clf_results_file}'\n",
        "    with open(node_clf_results_file, 'a') as f:\n",
        "\n",
        "      f.write(f\"{ef.split('/')[-1]} \\n\")\n",
        "      print(ef.split('/')[-1])\n",
        "\n",
        "      for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "        for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "          clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                          clf=LogisticRegression(max_iter=10000))\n",
        "\n",
        "          # Ensure X and Y are not empty before splitting and training\n",
        "          if X and Y:\n",
        "            result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "            # Results\n",
        "            li1.append(result['micro'])\n",
        "            li2.append(result['macro'])\n",
        "          else:\n",
        "            print(f\"Skipping classification for ratio {clf_ratio[i]} due to empty training data.\")\n",
        "            # Handle the case where X or Y is empty, perhaps by appending a placeholder or skipping the loop\n",
        "            continue\n",
        "\n",
        "\n",
        "        if li1 and li2: # Only calculate mean if there were valid results\n",
        "            mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "            ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "            print(mi)\n",
        "            print(ma)\n",
        "            print()\n",
        "\n",
        "            f.writelines(str(str(mi)+str(ma)))\n",
        "            f.write('\\n')\n",
        "\n",
        "        # Reinitialize the dictionaries and lists for the next ratio\n",
        "        mi = {}\n",
        "        ma = {}\n",
        "        li1 = []\n",
        "        li2 = []\n",
        "\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_vector[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYoibP4JSQAa",
        "outputId": "77d45786-e689-4935-d9f2-83a10f079f37"
      },
      "id": "aYoibP4JSQAa",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.31182464957237244,\n",
              " 0.21451017260551453,\n",
              " 0.2732299566268921,\n",
              " 0.39871835708618164,\n",
              " 0.05995843559503555,\n",
              " 0.3546711504459381,\n",
              " 0.12812590599060059,\n",
              " -0.1245405524969101,\n",
              " -0.27354884147644043,\n",
              " -0.24918422102928162,\n",
              " 0.20510736107826233,\n",
              " -0.08421233296394348,\n",
              " 0.10644889622926712,\n",
              " -0.477530837059021,\n",
              " 0.30905282497406006,\n",
              " -0.10331058502197266,\n",
              " 0.18018591403961182,\n",
              " -0.13134506344795227,\n",
              " -0.2851117253303528,\n",
              " 0.18426279723644257,\n",
              " -0.2793470025062561,\n",
              " -0.07107841968536377,\n",
              " 0.06437459588050842,\n",
              " -0.15468621253967285,\n",
              " 0.17518487572669983,\n",
              " 0.2506767213344574,\n",
              " 0.1730833649635315,\n",
              " 0.21652404963970184,\n",
              " -0.2581837475299835,\n",
              " 0.05304461345076561,\n",
              " -0.004364341497421265,\n",
              " -0.039489053189754486,\n",
              " 0.0002693086862564087,\n",
              " -0.06794948875904083,\n",
              " -0.15037985146045685,\n",
              " -0.3273056149482727,\n",
              " -0.3928309679031372,\n",
              " -0.2881232798099518,\n",
              " 0.07527810335159302,\n",
              " -0.1742650717496872,\n",
              " -0.1670026183128357,\n",
              " -0.09136922657489777,\n",
              " -0.0643187090754509,\n",
              " 0.057933464646339417,\n",
              " 0.22089439630508423,\n",
              " -0.15018409490585327,\n",
              " -0.1037401333451271,\n",
              " -0.23625703155994415,\n",
              " -0.09647293388843536,\n",
              " -0.2363405078649521,\n",
              " -0.0875244140625,\n",
              " 0.35333019495010376,\n",
              " -0.2850351929664612,\n",
              " -0.3798304796218872,\n",
              " 0.17841583490371704,\n",
              " 0.2025083601474762,\n",
              " -0.02081555873155594,\n",
              " 0.25914591550827026,\n",
              " -0.05143948644399643,\n",
              " -0.024163350462913513,\n",
              " -0.10086104273796082,\n",
              " -0.040784381330013275,\n",
              " 0.34266746044158936,\n",
              " -0.16550078988075256,\n",
              " -0.23742511868476868,\n",
              " 0.2098408341407776,\n",
              " -0.12220774590969086,\n",
              " -0.20547831058502197,\n",
              " -0.0880686491727829,\n",
              " 0.12179319560527802,\n",
              " 0.04309631884098053,\n",
              " -0.2811987102031708,\n",
              " 0.06025061756372452,\n",
              " -0.12224578112363815,\n",
              " 0.3952041566371918,\n",
              " 0.059911902993917465,\n",
              " -0.3416149914264679,\n",
              " -0.2668232321739197,\n",
              " 0.27852287888526917,\n",
              " 0.0740758627653122,\n",
              " -0.09443429857492447,\n",
              " 0.25845831632614136,\n",
              " 0.31727004051208496,\n",
              " 0.3205574154853821,\n",
              " 0.17190289497375488,\n",
              " 0.015085127204656601,\n",
              " -0.08619841933250427,\n",
              " 0.0760384127497673,\n",
              " 0.05194598436355591,\n",
              " 0.10863019526004791,\n",
              " -0.12666721642017365,\n",
              " -0.05629018694162369,\n",
              " -0.1677723079919815,\n",
              " -0.013058343902230263,\n",
              " -0.36484384536743164,\n",
              " 0.15099428594112396,\n",
              " 0.04986691474914551,\n",
              " 0.3182900547981262,\n",
              " -0.1258777230978012,\n",
              " -0.04377157986164093,\n",
              " 0.15367430448532104,\n",
              " 0.15367430448532104,\n",
              " 0.15367430448532104,\n",
              " 0.15367430448532104,\n",
              " 0.15367430448532104,\n",
              " 0.15367430448532104,\n",
              " 0.15367430448532104,\n",
              " 0.15367430448532104,\n",
              " 0.09927032887935638,\n",
              " 0.09927032887935638,\n",
              " 0.09927032887935638,\n",
              " 0.09927032887935638,\n",
              " 0.19091828167438507,\n",
              " 0.19091828167438507,\n",
              " 0.19091828167438507,\n",
              " 0.19091828167438507,\n",
              " 0.18893963098526,\n",
              " 0.18893963098526,\n",
              " 0.18893963098526,\n",
              " 0.18893963098526,\n",
              " 0.18893963098526,\n",
              " 0.18893963098526,\n",
              " 0.18893963098526,\n",
              " 0.18893963098526,\n",
              " 0.3276346027851105,\n",
              " 0.3276346027851105,\n",
              " 0.3276346027851105,\n",
              " 0.3276346027851105,\n",
              " 0.07346789538860321,\n",
              " 0.07346789538860321,\n",
              " 0.07346789538860321,\n",
              " 0.07346789538860321,\n",
              " 0.07346789538860321,\n",
              " 0.07346789538860321,\n",
              " 0.07346789538860321,\n",
              " 0.07346789538860321,\n",
              " 0.0735429897904396,\n",
              " 0.0735429897904396,\n",
              " 0.0735429897904396,\n",
              " 0.0735429897904396,\n",
              " 0.0735429897904396,\n",
              " 0.0735429897904396,\n",
              " 0.0735429897904396,\n",
              " 0.0735429897904396,\n",
              " 0.08103901892900467,\n",
              " 0.08103901892900467,\n",
              " 0.08103901892900467,\n",
              " 0.08103901892900467,\n",
              " 0.0783410370349884,\n",
              " 0.0783410370349884,\n",
              " 0.0783410370349884,\n",
              " 0.0783410370349884,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " 0.078341044485569,\n",
              " -0.20833800733089447,\n",
              " -0.20833800733089447,\n",
              " -0.20833800733089447,\n",
              " -0.20833800733089447,\n",
              " -0.16161538660526276,\n",
              " -0.16161538660526276,\n",
              " -0.16161538660526276,\n",
              " -0.16161538660526276,\n",
              " -0.16156768798828125,\n",
              " -0.16156768798828125,\n",
              " -0.16156768798828125,\n",
              " -0.16156768798828125,\n",
              " -0.16156768798828125,\n",
              " -0.16156768798828125,\n",
              " -0.16156768798828125,\n",
              " -0.16156768798828125,\n",
              " -0.16156736016273499,\n",
              " -0.16156736016273499,\n",
              " -0.16156736016273499,\n",
              " -0.16156736016273499,\n",
              " -0.21296006441116333,\n",
              " -0.21296006441116333,\n",
              " -0.21296006441116333,\n",
              " -0.21296006441116333,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426,\n",
              " 0.01627141423523426]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mm9-5x6OSRmj"
      },
      "id": "Mm9-5x6OSRmj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}