{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa86248",
   "metadata": {},
   "source": [
    "# ***Libraries & Tools***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import scipy.cluster\n",
    "import gc\n",
    "import math\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.sparse import sparse_dense_matmul as sd_matmul\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Optional: Set GPU memory growth to avoid over-allocation\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU is ready for use!\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is TensorFlow using GPU?\", tf.test.is_built_with_cuda())  # Should return True\n",
    "print(\"GPU device:\", tf.test.gpu_device_name())  # Should return something like /device:GPU:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f05ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149d36a",
   "metadata": {},
   "source": [
    "# ***Variables & General Functionality***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name    = \"arxiv\"\n",
    "data_text_file  = \"data-v3-500.txt\" # For single execution\n",
    "data_text_files = [\"data-v3-500.txt\", \"data-v3-500C.txt\", \"YAKE10.txt\", \"YAKE5.txt\", \"RAKE10.txt\", \"RAKE5.txt\", \"RAKE10C.txt\", \"RAKE5C.txt\", \"TFIDF10.txt\", \"TFIDF5.txt\", \"PosR5.txt\",\n",
    "                   \"PosR10.txt\", \"TextR5.txt\", \"TextR10.txt\", \"TopicR5.txt\", \"TopicR10.txt\"]\n",
    "\n",
    "graph_file             = 'graph.txt' # For node classification\n",
    "parent_path            = f'Datasets/{dataset_name}/graph-v2'\n",
    "log_file               = 'CANE_Execution_Logs.txt'\n",
    "link_pred_results_file = 'CANE_Link_Pred_Res.txt'\n",
    "node_clf_results_file  = 'CANE_Node_Clf_Res.txt'\n",
    "categories_file        = 'group-v2.txt'\n",
    "model_name = 'DetGP'\n",
    "\n",
    "\n",
    "split_graph_file  = 'sgraph15.txt' # For single execution\n",
    "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
    "\n",
    "test_graph_file   = 'tgraph85.txt' # For single execution\n",
    "test_graph_files  = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75273cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for node classification\n",
    "clf_ratio = [0.15, 0.45, 0.75]\n",
    "clf_num = 5\n",
    "train_classifier = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "MAX_LENS = []\n",
    "neg_table_size = 1000000\n",
    "NEG_SAMPLE_POWER = 0.75\n",
    "batch_size = 64 # Orig: 128\n",
    "num_epoch = 50\n",
    "embed_size = 200\n",
    "lr = 1e-3\n",
    "inducing_num = 20\n",
    "report_epoch_num = 1\n",
    "eval_epoch_num = 10 * report_epoch_num\n",
    "random_seed = 42\n",
    "\n",
    "GPU_USAGE = 0.5\n",
    "GPU_ID = 0\n",
    "\n",
    "encoder_type = 'wavg'  # 'dmte' 'wavg'\n",
    "kernel_type = 'linear'\n",
    "trans_order = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ceccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average number of words from each data text file\n",
    "for txtf in data_text_files: # 1) ['data.txt'] 2) data_text_files:\n",
    "    total_word_count = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            total_word_count += len(re.findall(r\"\\b\\w+\\b\", line))\n",
    "            total_lines += 1\n",
    "\n",
    "    mean_word_count = total_word_count / total_lines if total_lines > 0 else 0\n",
    "    MAX_LENS.append(int(math.ceil(mean_word_count)))\n",
    "    print(f'=== {txtf} ===')\n",
    "    print(\"Mean word count:\", math.ceil(mean_word_count))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dac2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = MAX_LENS[-1] # For single execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_list = []\n",
    "for i in range(0, embed_size):\n",
    "    zero_list.append(0)\n",
    "zero_list = np.array(zero_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_from_file(file_path):\n",
    "  vectors = {}\n",
    "\n",
    "  with open(f'{file_path}', \"r\") as f:\n",
    "      for idx, line in enumerate(f):\n",
    "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
    "          vectors[idx] = vector  # Assign embedding to node idx\n",
    "\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775a009",
   "metadata": {},
   "source": [
    "# ***Tools***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_to_undirect(node_num, edges):\n",
    "    # When given edge [i,j] build undirect graph with edge [i,j] and [j,i], each edge only appear once\n",
    "    # Create unique hash codes for the nodes in the edges\n",
    "    edge_hash = []\n",
    "    for edge in edges:\n",
    "        edge_hash.append(edge[0]*node_num+edge[1])\n",
    "        edge_hash.append(edge[1]*node_num+edge[0])\n",
    "    \n",
    "    edge_hash = list(set(edge_hash)) # edge_hash will have unique hash codes for the edges\n",
    "    ud_edges = [[v // node_num, v % node_num] for v in edge_hash]\n",
    "    return ud_edges\n",
    "\n",
    "def get_initial_inducing(sess, model, inducing_num):\n",
    "    text_feature = sess.run(model.text_feature)\n",
    "    inducing_points = scipy.cluster.vq.kmeans2(text_feature, inducing_num, minit='points')[0]\n",
    "    return inducing_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b2c562",
   "metadata": {},
   "source": [
    "# ***Negative Sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitNegTable(edges):\n",
    "    a_list, b_list = zip(*edges)\n",
    "    a_list = list(a_list)\n",
    "    b_list = list(b_list)\n",
    "    node = a_list\n",
    "    node.extend(b_list)\n",
    "\n",
    "    node_degree = {}\n",
    "    for i in node:\n",
    "        if i in node_degree:\n",
    "            node_degree[i] += 1\n",
    "        else:\n",
    "            node_degree[i] = 1\n",
    "    sum_degree = 0\n",
    "    for i in node_degree.values():\n",
    "        sum_degree += pow(i, 0.75)\n",
    "\n",
    "    por = 0\n",
    "    cur_sum = 0\n",
    "    vid = -1\n",
    "    neg_table = []\n",
    "    degree_list = list(node_degree.values())\n",
    "    node_id = list(node_degree.keys())\n",
    "    for i in range(neg_table_size):\n",
    "        if ((i + 1) / float(neg_table_size)) > por:\n",
    "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
    "            por = cur_sum / sum_degree\n",
    "            vid += 1\n",
    "        neg_table.append(node_id[vid])\n",
    "\n",
    "    return neg_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf22ca",
   "metadata": {},
   "source": [
    "# ***Dataloader***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, text_path, graph_path, edge_split_ratio=None):\n",
    "        #self.graph, self.train_graph, self.test_graph = self.load_graph(graph_path, edge_split_ratio)\n",
    "        text_file, graph_file = self.load(text_path, graph_path)\n",
    "        self.edges = self.load_edges(graph_file)\n",
    "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
    "        #self.train_edges = edges_to_undirect(self.num_nodes, self.train_graph.edges())\n",
    "        #self.test_edges = edges_to_undirect(self.num_nodes, self.test_graph.edges())\n",
    "        self.negative_table = InitNegTable(self.edges)\n",
    "    \n",
    "    def load(self, text_path, graph_path):\n",
    "        text_file = open(text_path, 'rb').readlines()\n",
    "        for a in range(0, len(text_file)):\n",
    "            text_file[a] = str(text_file[a])\n",
    "            \n",
    "        graph_file = open(graph_path, 'rb').readlines()\n",
    "\n",
    "        return text_file, graph_file\n",
    "\n",
    "    def load_edges(self, graph_file):\n",
    "        edges = []\n",
    "        for i in graph_file:\n",
    "            edges.append(list(map(int, i.strip().decode().split())))\n",
    "\n",
    "        print(\"Total load %d edges.\" % len(edges))\n",
    "\n",
    "        return edges\n",
    "\n",
    "    def get_adj_list(self, edges, node_num):\n",
    "        adj_list = []\n",
    "        for node in range(node_num):\n",
    "                nbr = [node]\n",
    "                for edge in edges:\n",
    "                        if node in edge:\n",
    "                                nbr += edge\n",
    "                adj_list.append(list(set(nbr)))\n",
    "        return adj_list\n",
    "\n",
    "\n",
    "    def load_graph(self, graph_path, edge_split_ratio):\n",
    "        total_graph = nx.Graph()\n",
    "        train_graph = nx.Graph()\n",
    "        test_graph = nx.Graph()\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        graph_file = open(graph_path, 'rb').readlines()\n",
    "        for line in graph_file:\n",
    "            edge = list(map(int, line.strip().decode().split()))\n",
    "            total_graph.add_edge(edge[0],edge[1])\n",
    "            \n",
    "            if np.random.uniform(0.0, 1.0) <= edge_split_ratio:\n",
    "                train_graph.add_edge(edge[0],edge[1])\n",
    "            else:\n",
    "                test_graph.add_edge(edge[0],edge[1])\n",
    "                \n",
    "        return total_graph, train_graph, test_graph\n",
    "\n",
    "    # Original load_text() method\n",
    "    \"\"\" def load_text(self, text_path):\n",
    "        text_file = open(text_path, 'rb').readlines()\n",
    "        vocab = learn.preprocessing.VocabularyProcessor(MAX_LEN)\n",
    "        text = np.array(list(vocab.fit_transform(text_file)))\n",
    "        num_vocab = len(vocab.vocabulary_)\n",
    "        num_nodes = len(text_file)\n",
    "        return text, num_vocab, num_nodes \"\"\"\n",
    "    \n",
    "    def load_text(self, text_file):\n",
    "        #text_file = open(text_path, 'rb').readlines()\n",
    "        #for a in range(0, len(text_file)): text_file[a] = str(text_file[a])\n",
    "\n",
    "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=None,  # Set a limit if needed\n",
    "            output_mode='int',\n",
    "            output_sequence_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "        text_data = [line.strip() for line in text_file]\n",
    "        vectorize_layer.adapt(text_data)\n",
    "        text = vectorize_layer(text_data).numpy()\n",
    "        return text, len(vectorize_layer.get_vocabulary()), len(text)\n",
    "\n",
    "    \n",
    "    def subgraph_edges(self, node_list):\n",
    "        subg_edges = self.train_graph.subgraph(node_list).edges()\n",
    "        return edges_to_undirect(self.num_nodes, subg_edges)\n",
    "\n",
    "    # Original negative_sampling() method\n",
    "    \"\"\" def negative_sampling(self, graph, edges):\n",
    "        node1, node2 = zip(*edges)\n",
    "        sample_edges = []\n",
    "        #np.random.seed(config.random_seed)\n",
    "        for i in range(len(edges)):\n",
    "            neg_node = random.choice(list(graph.nodes))\n",
    "            while neg_node in graph.neighbors(node1[i]):  \n",
    "                neg_node = random.choice(list(graph.nodes))\n",
    "            sample_edges.append([node1[i], node2[i], neg_node])\n",
    "        return sample_edges \"\"\"\n",
    "    \n",
    "    def negative_sampling(self, edges):\n",
    "        node1, node2 = zip(*edges)\n",
    "        sample_edges = []\n",
    "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)]\n",
    "        for i in range(len(edges)):\n",
    "            neg_node = func()\n",
    "            while node1[i] == neg_node or node2[i] == neg_node:\n",
    "                neg_node = func()\n",
    "            sample_edges.append([node1[i], node2[i], neg_node])\n",
    "\n",
    "        return sample_edges\n",
    "\n",
    "\n",
    "    def generate_batches(self, mode=None):\n",
    "        num_batch = len(self.train_edges) // batch_size\n",
    "        edges = self.train_edges\n",
    "\n",
    "        if mode == 'add':\n",
    "          num_batch += 1\n",
    "          edges.extend(edges[:(batch_size - len(self.edges) // batch_size)])\n",
    "\n",
    "        if mode != 'add':\n",
    "          random.shuffle(edges)\n",
    "\n",
    "        sample_edges = edges[:num_batch*batch_size]\n",
    "        sample_edges = self.negative_sampling(self.train_graph, sample_edges)\n",
    "\n",
    "        batches = [sample_edges[i * batch_size:(i + 1) * batch_size] for i in range(num_batch)]\n",
    "        return batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19bc1c",
   "metadata": {},
   "source": [
    "# ***Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_Kernel(feature_a, feature_b, gamma = 1000.):\n",
    "    f_a = tf.expand_dims(feature_a, 1)  #[node_num_a, 1, f_dim]\n",
    "    f_b = tf.expand_dims(feature_b, 0)  #[1, node_num_b, f_dim]\n",
    "    k_ab = tf.reduce_sum(tf.square(f_a-f_b), axis = 2)\n",
    "    return tf.exp(-k_ab / gamma)\n",
    "\n",
    "def Linear_Kernel(feature_a, feature_b):\n",
    "    k_ab = tf.matmul(feature_a, feature_b, transpose_b = True)\n",
    "    return k_ab + 1.\n",
    "\n",
    "\n",
    "def Polynomial_Kernel(feature_a, feature_b):\n",
    "    return Linear_Kernel(feature_a, feature_b)**2.\n",
    "\n",
    "\n",
    "def regularized_adj(adj):\n",
    "    return adj/tf.sparse.reduce_sum(adj, axis = 1, keepdims = True)\n",
    "\n",
    "\n",
    "def get_transition_matrix(Adj): # a sparse adjacency matrix\n",
    "    reg_Adj = tf.sparse.add(Adj, tf.sparse.eye(tf.shape(Adj)[0]))\n",
    "    return reg_Adj/tf.sparse.reduce_sum(reg_Adj,axis = 1, keepdims = True)\n",
    "\n",
    "\n",
    "def norm_trans(adj_mat):\n",
    "    return adj_mat / tf.sparse.reduce_sum(adj_mat, axis = 1, keepdims = True)\n",
    "    \n",
    "\n",
    "class DetGP():\n",
    "    def __init__(self, vocab_size, num_nodes, text_data, edges):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.kernel = Linear_Kernel\n",
    "        self.inducing_num = inducing_num\n",
    "        self.whiten = False\n",
    "        self.embedding_dim = embed_size // 2\n",
    "        self.trans_order = trans_order\n",
    "\n",
    "        with tf.name_scope('read_inputs') as scope:\n",
    "            self.text_all = tf.constant(text_data, dtype = tf.int32)\n",
    "            self.node_a_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'a_ids')\n",
    "            self.node_b_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'b_ids')\n",
    "            self.node_n_ids = tf.compat.v1.placeholder(tf.int32, [None], name = 'n_ids')\n",
    "\n",
    "            self.edges = tf.constant(edges, dtype=tf.int64)\n",
    "            self.adj_mat = tf.sparse.SparseTensor(\n",
    "                            self.edges, tf.ones(tf.shape(self.edges)[0]),\n",
    "                            dense_shape=[self.num_nodes, self.num_nodes])\n",
    "            self.trans_mat = get_transition_matrix(self.adj_mat)\n",
    "            #self.initial_inducing_points = tf.placeholder(tf.float32, [None, embed_size/2], name = 'inducing')\n",
    "\n",
    "        with tf.variable_scope('ggp') as scope:\n",
    "            self.inducing_points = tf.get_variable(name = 'inducing_points', shape = [self.inducing_num, self.embedding_dim])\n",
    "            self.q_mu            = tf.get_variable(name = 'embedding_mu', shape = [self.inducing_num, self.embedding_dim], initializer = tf.initializers.constant(0.1))\n",
    "            self.alpha           = tf.get_variable(name = 'alpha', shape = [trans_order + 1], initializer = tf.initializers.constant(0.))\n",
    "            self.al              = tf.nn.softmax(self.alpha)\n",
    "\n",
    "        with tf.name_scope('initialize_embedding') as scope:\n",
    "            self.text_embed =   tf.Variable(tf.truncated_normal([vocab_size, embed_size // 2], stddev=0.3), name = 'word_embedding')\n",
    "            # self.node_embed = tf.Variable(tf.truncated_normal([num_nodes, embed_size // 2], stddev=0.3))\n",
    "            # self.node_embed = tf.clip_by_norm(self.node_embed, clip_norm=1, axes=1)\n",
    "\n",
    "        with tf.name_scope('lookup_embeddings') as scope:\n",
    "            self.text_emb_lookup = tf.nn.embedding_lookup(self.text_embed, self.text_all)\n",
    "            self.text_feature = tf.reduce_mean(self.text_emb_lookup, axis=1)               \n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "\n",
    "    def load_inducing_points(self, inducing_points):\n",
    "        self.inducing_points = tf.assign(self.inducing_points, inducing_points)\n",
    "        \n",
    "        \n",
    "    def ggp_proces (self, features, z_features):\n",
    "        delta = 0.01\n",
    "        Kxz = self.kernel(features, z_features)\n",
    "        Kzz = self.kernel(z_features, z_features) +tf.eye(self.inducing_num) * delta\n",
    "        Kzz = tf.stop_gradient(Kzz)\n",
    "\n",
    "        if self.trans_order == 3:\n",
    "            Kxz_0 = Kxz\n",
    "            Kxz_1 = sd_matmul(self.trans_mat, Kxz)\n",
    "            Kxz_2 = sd_matmul(self.trans_mat, sd_matmul(self.trans_mat, Kxz))\n",
    "            Kxz_3 = sd_matmul(self.trans_mat, Kxz_2)\n",
    "            Kxz = self.al[0]*Kxz_0 + self.al[1]*Kxz_1 + self.al[2] * Kxz_2 + self.al[3]*Kxz_3\n",
    "            \n",
    "        Lz = tf.cholesky(Kzz)\n",
    "        \n",
    "        # Compute the projection matrix A\n",
    "        A = tf.matrix_triangular_solve(Lz, tf.transpose(Kxz), lower=True)\n",
    "        if not self.whiten:\n",
    "            A = tf.matrix_triangular_solve(tf.transpose(Lz), A, lower=False)\n",
    "        \n",
    "        # construct the conditional mean\n",
    "        fmean = tf.matmul(A, self.q_mu, transpose_a=True)        \n",
    "        return fmean\n",
    "\n",
    "\n",
    "    def compute_loss(self, emb_a, emb_b, emb_n):\n",
    "        positive_loss = tf.reduce_sum(emb_a * emb_b, axis = 1)\n",
    "        negative_loss = -tf.reduce_sum(emb_a * emb_n, axis = 1)\n",
    "        p_likeli = positive_loss - tf.math.softplus(positive_loss)     #log(sigmoid(x))\n",
    "        n_likeli = negative_loss - tf.math.softplus(negative_loss)\n",
    "\n",
    "        total_loss = -tf.reduce_mean(p_likeli + n_likeli)\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def get_distance(self, feature, ind_points):\n",
    "        feature_prime = tf.expand_dims(feature, axis = 1)\n",
    "        ind_points_prime = tf.expand_dims(ind_points, axis = 0)\n",
    "        dist = tf.reduce_mean(tf.square(feature_prime - ind_points_prime), axis = 2) #[feature_num, induc_num]\n",
    "        return tf.reduce_max(tf.sqrt(dist))\n",
    "\n",
    "    \n",
    "    def dmte_embeddings(self, text_features):\n",
    "        first_order_emb = text_features\n",
    "        second_orde_emb = sd_matmul(self.trans_mat, self.text_feature)\n",
    "\n",
    "        return first_order_emb + second_orde_emb\n",
    "\n",
    "    \n",
    "    def _build_model(self):\n",
    "        self.text_emb = self.dmte_embeddings(self.text_feature)\n",
    "        self.struct_emb = self.ggp_proces(self.text_emb, self.inducing_points)\n",
    "\n",
    "        self.text_emb_a = tf.nn.embedding_lookup(self.text_emb, self.node_a_ids)\n",
    "        self.text_emb_b = tf.nn.embedding_lookup(self.text_emb, self.node_b_ids)\n",
    "        self.text_emb_n = tf.nn.embedding_lookup(self.text_emb, self.node_n_ids)\n",
    "        self.text_loss  = self.compute_loss(self.text_emb_a, self.text_emb_b, self.text_emb_n)\n",
    "\n",
    "        self.struct_emb_a = tf.nn.embedding_lookup(self.struct_emb, self.node_a_ids)\n",
    "        self.struct_emb_b = tf.nn.embedding_lookup(self.struct_emb, self.node_b_ids)\n",
    "        self.struct_emb_n = tf.nn.embedding_lookup(self.struct_emb, self.node_n_ids)\n",
    "        self.struct_loss  = self.compute_loss(self.struct_emb_a, self.struct_emb_b, self.struct_emb_n)\n",
    "        \n",
    "        self.total_loss = self.text_loss + 0.3 * self.struct_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4929526b",
   "metadata": {},
   "source": [
    "# ***Classify***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a95f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return np.asarray(all_labels)\n",
    "\n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, vectors, clf):\n",
    "        self.embeddings = vectors\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        # X_train = [self.embeddings[x] for x in X]\n",
    "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\", \"macro\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y, Y_, average=average)\n",
    "        return results\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = np.random.get_state()\n",
    "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
    "        np.random.seed(seed)\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
    "\n",
    "        # Access the values of X and Y based on the shuffled indices\n",
    "\n",
    "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "\n",
    "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
    "        np.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9da28",
   "metadata": {},
   "source": [
    "# ***Run (Single Execution)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {}\n",
    "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
    "  labels = f.readlines()\n",
    "\n",
    "for la in labels:\n",
    "  label_dic[la.split()[0]] = la.split()[1:][0] # la.split()[0] = the node id ----- la.split()[1:][0] = The label of that node. If a node has many labels, take the first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_USAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db87599",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(f'{parent_path}/{data_text_file}', f'{parent_path}/{graph_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving embeddings\n",
    "#embed_file = f\"{parent_path}/Results/DetGP/embed_link_pred_{split_graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
    "embed_file = f\"{parent_path}/Results/DetGP/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\" # For node classification the whole graph ('graph.txt') is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess = tf.compat.v1.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "    with sess.as_default():\n",
    "        model = DetGP(data.num_vocab, data.num_nodes, data.text, data.edges)\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
    "        train_op = opt.minimize(model.total_loss)\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        inducing_points = get_initial_inducing(sess, model, inducing_num)\n",
    "        model.load_inducing_points(inducing_points)\n",
    "\n",
    "        # Training\n",
    "        start_time = datetime.now()\n",
    "        for epoch in range(num_epoch):\n",
    "            loss_epoch=0\n",
    "            batches=data.generate_batches()\n",
    "            num_batch=len(batches)\n",
    "            for i in range(num_batch):\n",
    "                batch=batches[i]\n",
    "\n",
    "                node1, node2, node3=zip(*batch)    \n",
    "                node1, node2, node3=np.array(node1),np.array(node2),np.array(node3)\n",
    "                # text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
    "                feed_dict={\n",
    "                    #model.edges: data.edges,\n",
    "                    #model.text_all: data.text,\n",
    "                    #model.inducing_points: inducing_points,\n",
    "                    model.node_a_ids: node1,\n",
    "                    model.node_b_ids: node2,\n",
    "                    model.node_n_ids: node3}\n",
    "\n",
    "                # run the graph \n",
    "                _, loss_batch, al  = sess.run([train_op, model.total_loss, model.al],feed_dict=feed_dict)\n",
    "                loss_epoch += loss_batch\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        print(f'Total time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
    "\n",
    "        text_emb, struct_emb = sess.run([model.text_emb_a, model.struct_emb_a], feed_dict = {\n",
    "            # model.edges: data.edges,\n",
    "            # model.text_all: data.text,\n",
    "            model.node_a_ids: np.arange(data.num_nodes) \n",
    "        })\n",
    "        \n",
    "        embed = np.concatenate((text_emb, struct_emb), axis=1)\n",
    "\n",
    "        with open(embed_file, 'wb') as f:\n",
    "            for i in range(data.num_nodes):\n",
    "                if embed[i]:\n",
    "                    f.write((' '.join(map(str, embed[i])) + '\\n').encode())\n",
    "                else:\n",
    "                    #f.write('\\n'.encode()) # For link prediction\n",
    "                    f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
